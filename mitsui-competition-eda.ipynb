{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":94771,"databundleVersionId":13044405,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Section 1: Initial Setup and Data Loading¬∂","metadata":{}},{"cell_type":"code","source":"# Essential imports for our EDA\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime\nfrom collections import Counter\nfrom typing import Optional, List\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# Set style and color palette\nsns.set_theme(style=\"darkgrid\")\nCOLOR_PALETTE = sns.color_palette(\"viridis\", n_colors=10)\n\nprint(\"üìä Starting Commodity Price Forecasting EDA\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:39:06.613665Z","iopub.execute_input":"2025-08-22T10:39:06.614424Z","iopub.status.idle":"2025-08-22T10:39:08.623075Z","shell.execute_reply.started":"2025-08-22T10:39:06.614392Z","shell.execute_reply":"2025-08-22T10:39:08.622429Z"}},"outputs":[{"name":"stdout","text":"üìä Starting Commodity Price Forecasting EDA\n==================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Memory Saving","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# ===================================================================================\n# ‚úÖ Complete Data Loading Cell for Mitsui Commodity Prediction Challenge\n# ===================================================================================\nimport os\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Utility to list files for debugging\nprint(\"Available files under /kaggle/input/mitsui-commodity-prediction-challenge:\")\nprint(os.listdir(\"/kaggle/input/mitsui-commodity-prediction-challenge\"))\n\n# Define paths\nBASE = \"/kaggle/input/mitsui-commodity-prediction-challenge\"\nTRAIN_CSV = f\"{BASE}/train.csv\"\nTEST_CSV = f\"{BASE}/test.csv\"\nLABELS_CSV = f\"{BASE}/train_labels.csv\"\nPAIRS_CSV = f\"{BASE}/target_pairs.csv\"\nLAGGED_DIR = f\"{BASE}/lagged_test_labels\"\n\n# ================================================================================\n# Memory optimization (NaN-safe)\ndef reduce_mem_usage(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        if df[col].isnull().all(): continue\n        col_type = df[col].dtype\n        c_min, c_max = df[col].min(skipna=True), df[col].max(skipna=True)\n        if col_type != object and col_type.name != 'category':\n            if str(col_type).startswith('int'):\n                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if not np.isnan(c_min) and not np.isnan(c_max):\n                    if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print(f\"Memory downcast: {start_mem:.2f} ‚Üí {end_mem:.2f} MB ({100*(start_mem-end_mem)/start_mem:.1f}% reduction)\")\n    return df\n\n# # ================================================================================\n# # Loading\n# train = reduce_mem_usage(pd.read_csv(TRAIN_CSV), verbose=True)\n# test = reduce_mem_usage(pd.read_csv(TEST_CSV), verbose=True)\n# labels = reduce_mem_usage(pd.read_csv(LABELS_CSV), verbose=True)\n# pairs = reduce_mem_usage(pd.read_csv(PAIRS_CSV), verbose=True)\n\n# # Load all lagged test labels\n# lagged = {}\n# for fname in sorted(os.listdir(LAGGED_DIR)):\n#     if fname.endswith(\".csv\"):\n#         lag = fname.replace(\"test_labels_lag_\", \"\").replace(\".csv\", \"\")\n#         path = os.path.join(LAGGED_DIR, fname)\n#         lagged[f\"lag_{lag}\"] = reduce_mem_usage(pd.read_csv(path), verbose=True)\n\n# print(\"Loaded lagged test label files:\", list(lagged.keys()))\n\n# # ================================================================================\n# # Confirm variables are defined\n# print(f\"train: {train.shape}, labels: {labels.shape}, pairs: {pairs.shape}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Without Memory saving\n##### Saw weird behaviors when calculating mean, var\n##### e.g.  train['LME_AH_Close'].var() comes out as inf","metadata":{}},{"cell_type":"code","source":"# Define paths\nBASE = \"/kaggle/input/mitsui-commodity-prediction-challenge\"\nTRAIN_CSV = f\"{BASE}/train.csv\"\nTEST_CSV = f\"{BASE}/test.csv\"\nLABELS_CSV = f\"{BASE}/train_labels.csv\"\nPAIRS_CSV = f\"{BASE}/target_pairs.csv\"\nLAGGED_DIR = f\"{BASE}/lagged_test_labels\"\n\n# ================================================================================\n# Loading\ntrain = pd.read_csv(TRAIN_CSV)\ntest = pd.read_csv(TEST_CSV)\nlabels = pd.read_csv(LABELS_CSV)\npairs = pd.read_csv(PAIRS_CSV)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Targets","metadata":{}},{"cell_type":"code","source":"def generate_log_returns(data: pd.Series, lag: int):\n    log_returns = pd.Series(np.nan, index=data.index)\n\n    # Compute log returns based on the rules\n    for t in range(len(data)):\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            try:\n                log_returns.iloc[t] = np.log(data.iloc[t + lag + 1] / data.iloc[t + 1])\n            except Exception:\n                log_returns.iloc[t] = np.nan\n    return log_returns\n\ndef generate_log_returns_dataframe(data: pd.DataFrame, lag: int):\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        try:\n            log_returns = np.log(data / data.shift(lag))\n        except Exception:\n            log_returns = data * np.nan\n    return log_returns\n\ndef generate_targets(column_a: pd.Series, column_b: pd.Series, lag: int) -> pd.Series:\n    a_returns = generate_log_returns(column_a, lag)\n    b_returns = generate_log_returns(column_b, lag)\n    return a_returns - b_returns\n\n# generate_targets(train['LME_CA_Close'], train['US_Stock_CCJ_adj_close'], 4)","metadata":{"_uuid":"0386e0a8-589b-4c7a-a717-feadacb9b39f","_cell_guid":"b2b824bd-8964-4401-83a0-55784a0983c7","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Understanding the timeseries data\n\ntrain_cols = [col.split(\"_\")[0] for col in train.columns if col != 'date_id']\n\ntrain_cols_counter = Counter(train_cols)\n\nprint(\"=\" * 50)\nprint(\"\\n# Instrument Categories:\")\nprint(\"\\n\".join([f\"{idx}. {item[0]}: {item[1]} instruments\" for idx, item in enumerate(train_cols_counter.items())]))\n\nprint(\"\\n\\n\", \"=\"*50)\nprint(\"\\n# Sample Instruments:\")\nfor col_category in train_cols_counter:\n    print(col_category, \" :\")\n    print(\" , \".join([col for col in train.columns if col.startswith(col_category)][:10]))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def missing_value_analysis(df: pd.DataFrame, title):\n    print(f\"\\nüîé MISSING VALUE ANALYSIS: {title}\")\n    df_missing = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False).apply(lambda x: \"{:,.2f}%\".format(x))\n    display(df_missing.T)\n    \n\ndef plot_feature_distributions(df: pd.DataFrame, cols: Optional[List[str]] = None,\n                             n: int = 12, bins: int = 60, title: Optional[str] = None):\n    if cols is None:\n        # pick top features with max variance\n        # Features with larger variances generally contain more information\n        cols = df.select_dtypes(include=np.number).var().sort_values(ascending=False).index[:n].tolist()\n\n    ncols = 4\n    nrows = int(np.ceil(n / ncols))\n\n    fig, ax = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n    ax = ax.flatten()\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n        for i, col in enumerate(cols):\n            sns.histplot(df[col], bins=bins, kde=True, color=COLOR_PALETTE[i % len(COLOR_PALETTE)], ax=ax[i])\n\n            # calculate median, skewness, kurtosis\n            median = df[col].median()\n            skewness = df[col].skew()\n            kurtosis = df[col].kurt()\n            \n            ax[i].set_title(label=f\"median={np.round(median,2)}, skew={np.round(skewness,2)}, kurt={np.round(kurtosis,2)}\", fontsize=11)\n            ax[i].grid(alpha=0.3)\n\n        if title:\n            plt.suptitle(title)\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_target_distributions(df: pd.DataFrame, n: int = 12, bins: int = 60):\n    cols = [col for col in labels.columns if col.startswith(\"target_\")][:n]\n    plot_feature_distributions(df=df, cols=cols, n=n, title=\"Target Distributions of first 12 targets\")\n\ndef plot_feature_correlation_matrix(df: pd.DataFrame, cols: Optional[List[str]] = None,\n                                   n: int = 12, title: Optional[str] = None):\n    if cols is None:\n        # pick top features with max variance\n        # Features with larger variances generally contain more information\n        cols = df.select_dtypes(include=np.number).var().sort_values(ascending=False).index[:n].tolist()\n\n    corr = df[cols].corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    \n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n    \n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True, fmt=\".2f\",\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\ndef plot_target_feature_correlation_matrix(train: pd.DataFrame, labels: pd.DataFrame, pairs: pd.DataFrame,\n                                          n: int = 12, title: Optional[str] = None,\n                                          correlation_threshold: float = 0.70,\n                                          ax: plt.Axes = None):\n    \n    # Compute correlation\n    df1 = train[[col for col in train.columns if col != 'date_id']] \n    \n    # select label columns to run correlation on\n    select_cols = []\n\n    for lag in pairs['lag'].unique():\n        targets = pairs[pairs['lag'] == lag]['target']\n        random_cols = targets.iloc[(np.random.rand(10)*(len(targets)-1)).astype(int)].tolist()\n        select_cols.extend(random_cols)\n\n    select_cols = list(set(select_cols))\n    select_pairs = pairs.set_index('target').loc[select_cols]['pair'] + \"_lag_\" + pairs.set_index('target').loc[select_cols]['lag'].astype(str)\n    \n    corr = pd.DataFrame(index = df1.columns, columns=select_pairs)\n    \n    for idx, col in enumerate(select_cols):\n        corr.loc[df1.columns, select_pairs[idx]] = df1.corrwith(labels[col])\n\n    high_correlation_data = corr[corr.abs() > correlation_threshold].replace(0, np.nan)\n    high_correlation_data.dropna(how='all', axis=0, inplace=True)\n    high_correlation_data.dropna(how='all', axis=1, inplace=True)\n\n    if not(high_correlation_data.empty):\n        # add an extra column or row if the len of any = 1\n        if len(high_correlation_data.columns) == 1:\n            high_correlation_data[\"dummy\"] = 0\n        if len(high_correlation_data.index) == 1:\n            high_correlation_data.loc[\"dummy\"] = 0\n\n        high_correlation_data = high_correlation_data.astype(float)\n\n        if ax is None:\n            # Set up the matplotlib figure\n            f, axes = plt.subplots(figsize=(11, 9))\n    \n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n                                     \n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(high_correlation_data, cmap=cmap, vmax=.3, center=0, annot=True, fmt=\".2f\",\n                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n        \n        if ax is None:\n            plt.tight_layout()\n            plt.show()\n    else:\n        print(f\"\\n‚ùóno strong correlation found for the set threshold of {int(correlation_threshold*100)}%\\n\")\n\n        if ax is None:\n            # Set up the matplotlib figure\n            f, ax = plt.subplots(figsize=(11,9))\n\n        # stack correlation dataframe\n        corr_stack = corr.stack()\n        \n        # Display the histogram of all correlations\n        sns.histplot(corr_stack, kde=True, ax=ax)\n\n        # calculate median, skewness, kurtosis\n        median = corr_stack.median()\n        skewness = corr_stack.skew()\n        kurtosis = corr_stack.kurt()\n        \n        ax.set_title(label=f\"median={np.round(median,2)}, skew={np.round(skewness,2)}, kurt={np.round(kurtosis,2)}\", fontsize=11)\n        ax.grid(alpha=0.3)\n        \n        if ax is None:\n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_value_analysis(train, \"TRAIN\")\nmissing_value_analysis(test, \"TEST\")\nmissing_value_analysis(labels, \"TRAIN_LABELS\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n# üîéDistributions of train dataset with largest variances\\n\")\nplot = plot_feature_distributions(train)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n# üîéDistributions of train labels dataset with largest variances\\n\")\nplot_target_distributions(labels)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n# üîéCorrelation Matrix of top 12 features by variance \\n\")\nplot_feature_correlation_matrix(train)#, cols=['US_Stock_VYM_adj_close','LME_ZS_Close','US_Stock_VTV_adj_close'])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n# üîéCorrelation matrix for features with very high correlation\\n\")\n\ndef fetch_high_correlation_columns(df: pd.DataFrame, correlation_threshold: float):\n    correlation_data = train.corr()\n    \n    high_corelation_columns = []\n\n    # loop through correlation matrix to fetch the high correlation columns\n    for i in range(len(correlation_data.columns)-1):\n        for j in range(i+1, len(correlation_data.columns)):\n            coli = correlation_data.columns[i]\n            colj = correlation_data.columns[j]\n            if (np.abs(correlation_data.loc[coli, colj]) > correlation_threshold):\n                high_corelation_columns.extend([coli, colj])\n    \n    high_corelation_columns = list(set(high_corelation_columns))\n\n    return high_corelation_columns\n\ncols = fetch_high_correlation_columns(train, 0.99)\n\nplot_feature_correlation_matrix(train, cols=cols[:20])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nprint(f\"\\n# üîéTarget - Feature correlation matrix with a correlation threshold: {correlation_threshold}\\n\")\n\nplot_target_feature_correlation_matrix(train, labels, pairs, correlation_threshold=0.2)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nprint(\"\\nüîé Checking if taking log returns have any impact on the correlation distribution\")\n\nf, ax = plt.subplots(2,3, figsize=(11, 9))\n\nlog_returns = generate_log_returns_dataframe(train, 1)\nplot_target_feature_correlation_matrix(train, labels, pairs, correlation_threshold=0.5, ax=ax[0][0])\n\nlog_returns = generate_log_returns_dataframe(train, 4)\nplot_target_feature_correlation_matrix(train, labels, pairs, correlation_threshold=0.5, ax=ax[0][1])\n\nlog_returns = generate_log_returns_dataframe(train, 10)\nplot_target_feature_correlation_matrix(train, labels, pairs, correlation_threshold=0.5, ax=ax[0][2])\n\nlog_returns = generate_log_returns_dataframe(train, 20)\nplot_target_feature_correlation_matrix(train, labels, pairs, correlation_threshold=0.5, ax=ax[1][0])\n\nlog_returns = generate_log_returns_dataframe(train, 30)\nplot_target_feature_correlation_matrix(train, labels, pairs, correlation_threshold=0.5, ax=ax[1][1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Specific imports for this section\n\nimport networkx as nx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:39:12.757253Z","iopub.execute_input":"2025-08-22T10:39:12.757712Z","iopub.status.idle":"2025-08-22T10:39:13.138939Z","shell.execute_reply.started":"2025-08-22T10:39:12.757688Z","shell.execute_reply":"2025-08-22T10:39:13.138355Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def date_features(df):\n    df['dayofweek'] = df['date_id'] % 7\n    df['month'] = (df['date_id'] // 30) % 12\n    df['quarter'] = df['month'] // 3\n    df['day_of_month'] = df['date_id'] % 30\n\n    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n    df['is_month_start'] = (df['day_of_month'] == 0).astype(int)\n    df['is_month_end'] = (df['day_of_month'] == 29).astype(int)\n\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.fillna(0, inplace=True)\n    return df\n\ndef dispersion_of_correlated_instruments(df: pd.DataFrame, correlation_threshold: int):\n    corr = train[[col for col in train.columns if col != 'date_id']].corr()\n\n    # Construct a Graph with columns pairs as edges for pairs with high correlation\n    G = nx.Graph()\n    for i in corr.columns:\n        for j in corr.columns:\n            if i != j and np.abs(corr.loc[i, j]) > correlation_threshold:\n                G.add_edge(i, j)\n    \n    # Get connected components (each is a cohort)\n    cohorts = list(nx.connected_components(G))    \n    print(f\"\\nüåê Number of cohorts founds: {len(cohorts)}\\n\")\n\n    # Generate std, skew and kurtosis for each of those cohorts on a daily basis (axis=1)\n    dispersion_dict = {}\n    for idx, cohort in enumerate(cohorts):\n        group = train[list(cohort)]\n        dispersion_dict[f'Cohort_{idx}'] = pd.DataFrame({\n            'std': group.std(axis=1),\n            'skew': group.skew(axis=1),\n            'kurtosis': group.kurt(axis=1)  # or .kurtosis(axis=1)\n        })\n\n    return dispersion_dict\n\ndef add_dispersion_features(df: pd.DataFrame, correlation_threshold: int):\n    dispersion_dict = dispersion_of_correlated_instruments(df, correlation_threshold)\n    features = []\n    for cohort in dispersion_dict:\n        old_columns = df.columns\n        new_features = [f'{cohort}_std',f'{cohort}_skew',f'{cohort}_kurt']\n        features.extend(new_features)\n        new_columns = list(old_columns) + [f'{cohort}_std',f'{cohort}_skew',f'{cohort}_kurt']\n        df = pd.concat([df, dispersion_dict[cohort]], axis=1)\n        df.columns = new_columns\n    return df, features\n\ndef add_log_return_features(df: pd.DataFrame, lag: int = 1, columns: list = None):\n    if columns is None:\n        columns = [x for x in df.columns if x != 'date_id']\n    log_returns = np.log(df[columns] / df[columns].shift(lag))\n    features = [f'{x}_{lag}_log_ret' for x in columns]\n    log_returns.columns = features\n    return log_returns, features\n\ndef add_target_log_return_features(df: pd.DataFrame, pairs_df: pd.DataFrame, lag: int = 1):\n    # Step 1: Get all unique columns needed\n    all_pairs = pairs_df['pair'].str.split(' - ')\n    unique_columns = set([item for sublist in all_pairs for item in sublist])\n\n    # Step 2: Precompute log returns for all unique columns\n    log_returns, _ = add_log_return_features(df, lag, list(unique_columns))\n    \n    # Step 3: Compute pairwise differences\n    features = []\n    result_dict = {}\n    for idx, row in pairs_df.drop_duplicates(subset=['target', 'pair']).iterrows():\n        target = row['target']\n        pair = row['pair'].split(' - ')\n        if len(pair) == 1:\n            diff = log_returns[f'{pair[0]}_{lag}_log_ret']\n        elif len(pair) == 2:\n            diff = log_returns[f'{pair[0]}_{lag}_log_ret'] - log_returns[f'{pair[1]}_{lag}_log_ret']\n        else:\n            continue\n        feat_name = f'{target}_{lag}_log_ret_diff'\n        result_dict[feat_name] = diff\n        features.append(feat_name)\n    result_df = pd.DataFrame(result_dict)\n    return result_df, features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:41:02.634868Z","iopub.execute_input":"2025-08-22T10:41:02.636013Z","iopub.status.idle":"2025-08-22T10:41:02.649292Z","shell.execute_reply.started":"2025-08-22T10:41:02.635984Z","shell.execute_reply":"2025-08-22T10:41:02.648423Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def plot_dispersion_dict(dispersion_dict):\n    fig, ax = plt.subplots(figsize=(11,9))\n    colors = plt.cm.twilight_shifted(np.linspace(0, 1, len(dispersion_dict)))\n    for i, (cohort, df) in enumerate(dispersion_dict.items()):\n        if i == 0:\n            df['std'].plot(ax=ax, label=cohort, color=colors[i])\n        else:\n            ax_i = ax.twinx()\n            df['std'].plot(ax=ax_i, label=cohort, color=colors[i])\n                    \n            # Hide extra y-axis\n            ax_i.get_yaxis().set_visible(False)\n            ax_i.spines[\"right\"].set_visible(False)\n    \n    # Show only one legend\n    lines, labels = ax.get_legend_handles_labels()\n    ax.legend(lines, labels, loc='upper left')\n    \n    # plt.legend()\n    plt.tight_layout()\n    plt.title(\"Daily Standard Deviation per Cohort\")\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nüåê Analysing dispersion of correlated instruments\\n\")\n\ndispersion_dict = dispersion_of_correlated_instruments(train, 0.95)\nplot_dispersion_dict(dispersion_dict)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nüåê Target - feature analysis of dispersion (std) of cohort_0 with all targets\\n\")\n\nindex = dispersion_dict['Cohort_0']['std'].index\n\ndispersion_df_std = pd.DataFrame([df['std'] for cohort, df in dispersion_dict.items()], index=dispersion_dict.keys(), columns=index).transpose()\n\nplot_target_feature_correlation_matrix(dispersion_df_std, labels, pairs, correlation_threshold=0.5)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# specific imports for training\n\n# Utility imports\nimport os, gc, warnings, random, pickle\nfrom pathlib import Path\n\n# Booster imports\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n# Validation imports\nfrom sklearn.model_selection import KFold\n\n# data imports\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nfrom tqdm.auto import tqdm\nimport torch\nimport kaggle_evaluation.mitsui_inference_server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:41:06.159124Z","iopub.execute_input":"2025-08-22T10:41:06.159436Z","iopub.status.idle":"2025-08-22T10:41:06.164311Z","shell.execute_reply.started":"2025-08-22T10:41:06.159413Z","shell.execute_reply":"2025-08-22T10:41:06.163504Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ====================================================\n# Config\n# ====================================================\nclass Config:\n    AUTHOR = 'mitsui_ai'\n    VERSION = 2\n    SEED = 42\n    N_FOLDS = 3\n    BOOSTERS = ['lgbm', 'xgb', 'cat']\n    MAX_ROUNDS = 2500\n    EARLY_STOP = 100\n    VERBOSE = 1\n    DATA_DIR = Path('/kaggle/input/mitsui-commodity-prediction-challenge')\n    PROCESSED_DATA_DIR = Path(f'./version{VERSION}/processed_data'); os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n    MODEL_DIR = Path(f'./version{VERSION}/models'); os.makedirs(MODEL_DIR, exist_ok=True)\n    OOF_DIR = Path(f'./version{VERSION}/oof'); os.makedirs(OOF_DIR, exist_ok=True)\n    TARGET_COUNT = 424\n    FEATURES_TO_ADD = ['target_id']\n\n    LGBM_PARAMS = {\n        'objective': 'regression', 'metric': 'rmse',\n        'learning_rate': 0.005, 'num_leaves': 8, 'seed': SEED,\n        'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0,\n    }\n\n    XGB_PARAMS = {\n        'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n        'learning_rate': 0.005, 'max_depth': 4, 'random_state': SEED,\n        'tree_method': \"hist\", 'device': \"cuda\"\n        # 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor',\n    }\n\n    CAT_PARAMS = {\n        'loss_function': 'RMSE', 'learning_rate': 0.005,\n        'iterations': MAX_ROUNDS, 'depth': 4,\n        'random_seed': SEED, 'verbose': False,\n        'task_type': 'GPU', 'devices': '0:1',\n    }\n    \n    HGB_PARAMS = {\n        \"max_iter\": MAX_ROUNDS,\n        \"random_state\": SEED,\n        \"early_stopping\": True,  # Stops early if no improvement\n        \"n_iter_no_change\": EARLY_STOP,  # Speed up convergence\n    }\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:41:32.067317Z","iopub.execute_input":"2025-08-22T10:41:32.068003Z","iopub.status.idle":"2025-08-22T10:41:32.076175Z","shell.execute_reply.started":"2025-08-22T10:41:32.067973Z","shell.execute_reply":"2025-08-22T10:41:32.075345Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# ====================================================\n# Seed and Utility\n# ====================================================\ndef set_seed(seed=Config.SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()\n\n# ====================================================\n# Train Booster\n# ====================================================\ndef train_model(booster, x_tr, y_tr, x_val, y_val):\n    # Handle missing values\n    y_tr = np.nan_to_num(y_tr, nan=0.0)\n    y_val = np.nan_to_num(y_val, nan=0.0)\n\n    if booster==\"lgbm\":\n        train_set = lgb.Dataset(x_tr, y_tr)\n        val_set = lgb.Dataset(x_val, y_val)\n        model = lgb.train(\n            Config.LGBM_PARAMS, train_set,\n            num_boost_round=Config.MAX_ROUNDS,\n            valid_sets=[val_set],\n            callbacks=[\n                lgb.early_stopping(Config.EARLY_STOP),\n                lgb.log_evaluation(Config.VERBOSE)\n            ]\n        )\n        return model, model.predict(x_val)\n    \n    elif booster=='xgb':\n        train_d = xgb.DMatrix(x_tr, label=y_tr)\n        valid_d = xgb.DMatrix(x_val, label=y_val)\n        model = xgb.train(\n            Config.XGB_PARAMS, train_d,\n            num_boost_round=Config.MAX_ROUNDS,\n            evals=[(valid_d, 'eval')],\n            early_stopping_rounds=Config.EARLY_STOP,\n            verbose_eval=Config.VERBOSE\n        )\n        return model, model.predict(xgb.DMatrix(x_val))\n\n    elif booster=='cat':\n        train_pool = Pool(x_tr, label=y_tr)\n        valid_pool = Pool(x_val, label=y_val)\n        model = CatBoostRegressor(**Config.CAT_PARAMS)\n        model.fit(train_pool, eval_set=valid_pool,\n                  early_stopping_rounds=Config.EARLY_STOP)\n        return model, model.predict(x_val)\n\n    elif booster=='histgb':\n        model = HistGradientBoostingRegressor(**Config.HGB_PARAMS)\n        model.fit(x_tr , y_tr)\n        return model, model.predict(y_val)\n\n# ====================================================\n# Training CV Wrapper\n# ====================================================\ndef run_cv(booster, df, features):\n    df = df.copy()\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(subset=['target'], inplace=True)\n\n    oof_preds = np.zeros(len(df))\n    kf = KFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n\n    np.save(Config.OOF_DIR / f'target.npy', df['target'].to_numpy())\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        x_tr = df.iloc[train_idx][features]\n        y_tr = df.iloc[train_idx]['target']\n        x_val = df.iloc[val_idx][features]\n        y_val = df.iloc[val_idx]['target']\n\n        model, val_preds = train_model(booster, x_tr, y_tr, x_val, y_val)\n        oof_preds[val_idx] = val_preds\n\n        with open(Config.MODEL_DIR / f'{booster}_fold{fold}.pkl', 'wb') as f:\n            pickle.dump(model, f)\n\n        del model, x_tr, y_tr, x_val, y_val\n        gc.collect()\n\n    np.save(Config.OOF_DIR / f'oof_{booster}.npy', oof_preds)\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:41:35.351219Z","iopub.execute_input":"2025-08-22T10:41:35.351891Z","iopub.status.idle":"2025-08-22T10:41:35.362587Z","shell.execute_reply.started":"2025-08-22T10:41:35.351861Z","shell.execute_reply":"2025-08-22T10:41:35.361685Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"%%time\n# ====================================================\n# Load Data\n# ====================================================\ntrain_df = pl.read_csv(Config.DATA_DIR / 'train.csv').to_pandas()\nlabel_df = pl.read_csv(Config.DATA_DIR / 'train_labels.csv').to_pandas()\npairs_df = pl.read_csv(Config.DATA_DIR / 'target_pairs.csv').to_pandas()\n\nfeatures = list(train_df.columns[1:]) + Config.FEATURES_TO_ADD\n\ndef prepare_data(X: pd.DataFrame, Y: pd.DataFrame=None, testing: bool = False):\n    X_df = X.copy()\n    for col in X_df.columns:\n        if X_df[col].dtype == 'object':\n            X_df[col] = np.nan\n\n    # date features\n    X_df = date_features(X_df)\n\n    # target log return features\n    log_returns_diff, lag_1_log_return_diff_features = add_target_log_return_features(X_df, pairs_df, lag=1)\n    X_df = pd.concat([X_df, log_returns_diff], axis=1)\n\n    X_df_all = []\n    for tid in range(Config.TARGET_COUNT if testing else len(Y.columns[1:])):\n        temp = X_df.copy()\n        if Y is not None:\n            temp['target'] = Y.iloc[:, tid+1]\n            temp = temp.dropna(subset=['target'])\n        temp['target_id'] = tid\n        X_df_all.append(temp)\n\n    X_df = pd.concat(X_df_all, axis=0).reset_index(drop=True)\n    features_final = features + [\n        'dayofweek', 'month', 'quarter', 'day_of_month',\n        'is_weekend', 'is_month_start', 'is_month_end'\n        ] + lag_1_log_return_diff_features\n    \n    if testing:\n        ret_X_df = X_df[features_final]\n    else:\n        ret_X_df = X_df\n        \n    if Y is not None:\n        ret_Y_S = X_df['target']\n        return ret_X_df, features_final, ret_Y_S\n    else:\n        return ret_X_df, features_final, 0\n\n# df_all = []\n# for j, col in enumerate(label_df.columns[1:]):\n#     temp = train_df.copy()\n#     temp['target'] = label_df[col]\n#     temp['target_id'] = j\n#     temp = add_features(temp)\n#     temp = temp.dropna(subset=['target'])\n#     df_all.append(temp)\n\n# train_full = pd.concat(df_all, axis=0).reset_index(drop=True)\ntrain_full, features_final, Y_full = prepare_data(train_df, label_df, testing=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T10:41:38.633367Z","iopub.execute_input":"2025-08-22T10:41:38.633769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# ====================================================\n# base line RMSE\n# ====================================================\nprint(f\"Target for RMSE to be lower than: \", Y_full.std())\n\n# ====================================================\n# Train All Boosters\n# ====================================================\nfor booster in Config.BOOSTERS:\n    print(f\"\\n[Training {booster.upper()}]\")\n    run_cv(booster, train_full, features_final)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Understanding the trained models","metadata":{}},{"cell_type":"code","source":"def model_analysis(target: pd.Series, predictions: pd.Series, model_name: str = \"\"):\n    f, ax = plt.subplots(2,2,figsize=(11,9))\n    \n    # Distribution of actual target values\n    sns.histplot(target.to_list(), kde=True, ax=ax[0][0])\n    ax[0][0].set_title('Target Distribution')\n    \n    ## Distribution of OOF predictions (takes too long)\n    # sns.histplot(predictions, kde=False, ax=ax[0][1])\n    \n    ax[0][1].hist(predictions, bins=100, alpha=0.7, density=True)\n    ax[0][1].set_title(f'{model_name} OOF Predictions Distribution')\n    \n    # Scatter plot: Actual vs Predicted\n    ax[1][0].scatter(target, predictions, alpha=0.6)\n    ax[1][0].set_xlabel('Actual Target')\n    ax[1][0].set_ylabel(f'{model_name} OOF Predictions')\n    ax[1][0].plot([target.min(), target.max()], [target.min(), target.max()], 'r--', alpha=0.8)\n    ax[1][0].set_title('Actual vs Predicted')\n    \n    # Residuals plot\n    residuals = target - predictions\n    ax[1][1].scatter(predictions, residuals, alpha=0.6)\n    ax[1][1].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n    ax[1][1].set_xlabel(f'{model_name} OOF Predictions')\n    ax[1][1].set_ylabel('Residuals')\n    ax[1][1].set_title('Residuals Plot')\n    \n    # Overall title\n    f.suptitle(f'{model_name} Model Analysis', fontsize=16, y=0.98)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate and display metrics\n    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n    import numpy as np\n    \n    mse = mean_squared_error(target, predictions)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(target, predictions)\n    r2 = r2_score(target, predictions)\n    \n    print(f\"\\n{model_name} OOF Metrics:\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n    print(f\"R¬≤ Score: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_cat = pd.Series(np.load(Config.OOF_DIR / \"oof_cat.npy\"), dtype='float64')\noof_lgbm = pd.Series(np.load(Config.OOF_DIR / \"oof_lgbm.npy\"), dtype='float64')\noof_xgb = pd.Series(np.load(Config.OOF_DIR / \"oof_xgb.npy\"), dtype='float64')\n\ntarget = pd.Series(np.load(Config.OOF_DIR / \"target.npy\"), dtype='float64')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nmodel_analysis(target, oof_cat, \"CatBoost\")\nmodel_analysis(target, oof_lgbm, \"LGBM\")\nmodel_analysis(target, oof_xgb, \"XGB\")\nmodel_analysis(target, (oof_cat + oof_lgbm + oof_xgb) / 3, \"XGB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}